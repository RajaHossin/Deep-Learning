{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " 1 and  2)\tWrite a custom code(function)  ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iPNTkdv5C626"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import random\n",
        "np.random.seed(10)\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oesVDlB0LWyH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Smooth Relu Activation Function\n",
        "def swish(x):\n",
        "    return x/(1+np.exp(-x))\n",
        "\n",
        "# Smooth Relu derivative\n",
        "def der_swish(x):\n",
        "    return 1/(1+np.exp(-x))-(x*np.exp(-x))/((1+np.exp(-x))*(1+np.exp(-x))) \n",
        "\n",
        "\n",
        "# Smooth Relu Activation Function\n",
        "def srelu(x):\n",
        "    return np.log(1+np.exp(x))\n",
        "\n",
        "# Smooth Relu derivative\n",
        "def der_srelu(x):\n",
        "    return (np.exp(x))/(x+np.exp(x))\n",
        "\n",
        "# Relu Activation Function\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "# htan derivative\n",
        "def der_relu(x):\n",
        "    return np.where(x>=0,1,0)\n",
        "\n",
        "def htan(x):\n",
        "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "\n",
        "# htan derivative\n",
        "def der_htan(x):\n",
        "    return 1 - htan(x) * htan(x)\n",
        "\n",
        "# Sigma  Activation Function\n",
        "def sigma(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# sigma derivative\n",
        "def der_sigma(x):\n",
        "    return sigma(x)*(1-sigma(x))\n",
        "    \n",
        "def initialize_params(layer_sizes):\n",
        "    params = {}\n",
        "    for i in range(1, len(layer_sizes)):\n",
        "        params['W' + str(i)] = np.random.randn(layer_sizes[i], layer_sizes[i-1])*0.01\n",
        "        params['B' + str(i)] = np.random.randn(layer_sizes[i],1)*0.01\n",
        "    return params\n",
        "\n",
        "def forward_propagation(X_train, params):\n",
        "    layers = len(params)//2\n",
        "    values = {}\n",
        "    for i in range(1, layers+1):\n",
        "        if i==1:\n",
        "            values['Z' + str(i)] = np.dot(params['W' + str(i)], X_train) + params['B' + str(i)]\n",
        "            values['A' + str(i)] = relu(values['Z' + str(i)])\n",
        "        else:\n",
        "            values['Z' + str(i)] = np.dot(params['W' + str(i)], values['A' + str(i-1)]) + params['B' + str(i)]\n",
        "            if i==layers:\n",
        "                values['A' + str(i)] = values['Z' + str(i)]\n",
        "            else:\n",
        "                values['A' + str(i)] = relu(values['Z' + str(i)])\n",
        "    return values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def backward_propagation(params, values, X_train, Y_train):\n",
        "    layers = len(params)//2\n",
        "    m = len(Y_train)\n",
        "\n",
        "\n",
        "    layers = len(values)//2\n",
        "    Y_pred = values['A' + str(layers)]\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    grads = {}\n",
        "    for i in range(layers,0,-1):\n",
        "        if i==layers:\n",
        "            # dA = 1/m * (values['A' + str(i)] - Y_train)\n",
        "            dA = 1/m * ((values['A' + str(i)] - Y_train))\n",
        "            dZ = dA\n",
        "        else:\n",
        "            dA = np.dot(params['W' + str(i+1)].T, dZ)\n",
        "            dZ = np.multiply(dA, der_relu(values['A' + str(i)]))\n",
        "        if i==1:\n",
        "            grads['W' + str(i)] = 1/m * np.dot(dZ, X_train.T)\n",
        "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "        else:\n",
        "            grads['W' + str(i)] = 1/m * np.dot(dZ,values['A' + str(i-1)].T)\n",
        "            grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    return grads\n",
        "\n",
        "def update_params(params, grads, learning_rate):\n",
        "    layers = len(params)//2\n",
        "    params_updated = {}\n",
        "    for i in range(1,layers+1):\n",
        "        params_updated['W' + str(i)] = params['W' + str(i)] - learning_rate * grads['W' + str(i)]\n",
        "        params_updated['B' + str(i)] = params['B' + str(i)] - learning_rate * grads['B' + str(i)]\n",
        "    return params_updated\n",
        "\n",
        "\n",
        "def model(X_train, X_test, Y_train, Y_test, layer_sizes, num_iters, learning_rate):\n",
        "    params = initialize_params(layer_sizes)\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        values = forward_propagation(X_train.T, params)\n",
        "        tr_cost = compute_cost(values, Y_train.T)\n",
        "        ts_values = forward_propagation(X_test.T, params)\n",
        "        ts_cost = compute_cost(ts_values, Y_test.T)\n",
        "        grads = backward_propagation(params, values,X_train.T, Y_train.T)\n",
        "        params = update_params(params, grads, learning_rate)\n",
        "        #print('Cost at iteration ' + str(i+1) + ' = ' + str(cost) + '\\n')\n",
        "        train_cost.append(tr_cost)\n",
        "        test_cost.append(ts_cost)\n",
        "    return params\n",
        "\n",
        "def compute_cost(values, y):\n",
        "    layers = len(values)//2\n",
        "    Y_pred = values['A' + str(layers)]\n",
        "\n",
        "    for i in range(len(y)-1):\n",
        "        delta_actual=y[i+1]-y[i]\n",
        "        delta_predicted=Y_pred[0][i+1]-Y_pred[0][i]\n",
        "\n",
        "    cost=np.sum(((Y_pred - y))*((Y_pred - y)))\n",
        "    return cost\n",
        "\n",
        "\n",
        "\n",
        "def predict(X, params):\n",
        "    values = forward_propagation(X.T, params)\n",
        "    predictions = values['A' + str(len(values)//2)].T\n",
        "    return predictions\n",
        "\n",
        "def train_test_cost_plot(train_cost,test_cost):\n",
        "    # line 1 points\n",
        "    x1 = [i for i in range (1,num_iters+1)]\n",
        "    y1 = train_cost[:]\n",
        "    # plotting the line 1 points\n",
        "    plt.plot(x1, y1, label = \"Train cost\")\n",
        "    # line 2 points\n",
        "    x2 = [i for i in range (1,num_iters+1)]\n",
        "    y2 = test_cost[:]\n",
        "    # plotting the line 2 points\n",
        "    plt.plot(x2, y2, label = \"Test cost\")\n",
        "    # naming the x axis\n",
        "    plt.xlabel('No of iteration')\n",
        "    # naming the y axis\n",
        "    plt.ylabel('Value of cost')\n",
        "    # giving a title to my graph\n",
        "    plt.title('Two cost on same graph!')\n",
        "    # show a legend on the plot\n",
        "    plt.legend()\n",
        "    \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #load dataset\n",
        "    df = pd.read_csv('file_pathname.csv') \n",
        "\n",
        "    #separate data into input and output features\n",
        "    X = np.array(df[['X1','X2','X3']].values.tolist())\n",
        "    # print(X.shape)\n",
        "    Y = np.array(df['Y2'].values.tolist())\n",
        "    \n",
        "    #split data into train and test sets in 80-20 ratio\n",
        "    train_data_len=int(df.shape[0]*0.8)\n",
        "    X_train=X[:train_data_len]\n",
        "    \n",
        "    X_test=X[train_data_len:]\n",
        "    Y_train=Y[:train_data_len]\n",
        "    Y_test =Y[train_data_len:]\n",
        "    # print(X_train,\"y\",Y_train)           \n",
        "\n",
        "    train_cost=[]\n",
        "    test_cost=[]\n",
        "\n",
        "\n",
        "    layer_sizes = [3,5,5, 1]                                                       #set layer sizes, do not change the size of the first and last layer \n",
        "    num_iters = 100000                                                    #set number of iterations over the training set(also known as epochs in batch gradient descent context)\n",
        "    learning_rate = 0.00001\n",
        "    alpha = 0                                                         #set learning rate for gradient descent\n",
        "    params = model(X_train, X_test, Y_train, Y_test, layer_sizes, num_iters, learning_rate)           #train the model\n",
        "\n",
        "    train_test_cost_plot(train_cost,test_cost)\n",
        "    predicted_train=predict(X_train,params)\n",
        "    predicted_test=predict(X_test,params)\n",
        "\n",
        "    \n",
        "y_predicted_test=predicted_test\n",
        "y_predicted_train=predicted_train\n",
        "# print(predicted_test)\n",
        "# print(predicted_train)"
      ],
      "metadata": {
        "id": "xQY0LmcyxG5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}